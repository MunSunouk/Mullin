{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mullin/model/svm.py\n",
    "\n",
    "from module.path_header import *  # 경로 정리해둔 헤더 파일\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "TRAIN_NAME = '0831-21-train.csv'\n",
    "LABEL_NAME = 'train_label.csv'\n",
    "TEST1_NAME = '0831-21-test1.csv'\n",
    "TEST2_NAME = '0831-21-test2.csv'\n",
    "MODEL1_NAME = 'svm_model1.pkl'\n",
    "MODEL2_NAME = 'svm_model2.pkl'\n",
    "\n",
    "TRAIN_PATH = os.path.join(PREPROCESS_DIR, TRAIN_NAME) \n",
    "LABEL_PATH = os.path.join(PREPROCESS_DIR, LABEL_NAME)\n",
    "TEST1_PATH = os.path.join(PREPROCESS_DIR, TEST1_NAME)\n",
    "TEST2_PATH = os.path.join(PREPROCESS_DIR, TEST2_NAME)\n",
    "MODEL1_PATH = os.path.join(MODEL_DIR, MODEL1_NAME)  # survival_time prediction model\n",
    "MODEL2_PATH = os.path.join(MODEL_DIR, MODEL2_NAME)  # amount_spent prediction model\n",
    "\n",
    "## main function\n",
    "# survival_time, amount_spent 에 대한 모델 각각 만들고 model/ 에 저장한다.\n",
    "# size=40000 (전체 train dataset) 으로 하면 시간 오래걸린다.\n",
    "def create_model_svm(train_X, train_y, size=1000):    \n",
    "    # train_test_split\n",
    "    train_X, val_X, train_y, val_y = train_test_split(train_X, train_y, train_size=0.8)     \n",
    "    \n",
    "    print('create survival time model')\n",
    "    survival_time_model(size, train_X, val_X, train_y, val_y)\n",
    "    print('create amount spent model')\n",
    "    amount_spent_model(size, train_X, val_X, train_y, val_y)\n",
    "    \n",
    "\n",
    "# svm 에 맞게 train input 형태 조정\n",
    "# 1~28 day 무시하고 acc_id 에 대한 값으로 squeeze\n",
    "def preprocess_X(train_X):\n",
    "    ## 합할 feature, 평균낼 feature 나누기\n",
    "    # to mean features\n",
    "    mean_features = ['isMajorClass', 'avg_play_rate_rank_per_p', 'tot_c_rank_per_p']\n",
    "    # to sum features\n",
    "    sum_features = train_X.columns.tolist()[2:]\n",
    "    for feat in mean_features:\n",
    "        sum_features.remove(feat)\n",
    "    # acc_id 에 대해 mean_features, sum_features 컬럼 평균/합 한 value들 concat 하기\n",
    "    # 의미: 1~28day 무시하고 feature들을 acc_id 에 대한 값으로 squeeze 하기\n",
    "    mean_pivot = train_X.pivot_table(index='acc_id', values=mean_features, aggfunc='mean')\n",
    "    sum_pivot = train_X.pivot_table(index='acc_id', values=sum_features, aggfunc='sum')\n",
    "    train_X = pd.concat((mean_pivot, sum_pivot), axis=1)        \n",
    "\n",
    "    # reset_index + acc_id 컬럼 지우기 (acc_id 인덱스, 따로 순서대로 저장해서 필요x)\n",
    "    train_X = train_X.reset_index(drop=True)\n",
    "\n",
    "    return train_X\n",
    "\n",
    "def preprocess_y_survival(train_y):\n",
    "    train_y = train_y.iloc[:,1]  # survival_time column 추출\n",
    "    # svm label 에 넣기 위해 스칼라 값으로 reshape\n",
    "    train_y = train_y.values.reshape(-1,) # reshape (40000, )\n",
    "    return train_y\n",
    "\n",
    "def preprocess_y_spent(train_y):\n",
    "    train_y = train_y.iloc[:,-1]  # amount_spent column 추출\n",
    "    # svm label 에 넣기 위해 스칼라 값으로 reshape\n",
    "    train_y = train_y.values.reshape(-1,) # reshape (40000, )\n",
    "    return train_y\n",
    "\n",
    "\n",
    "\n",
    "def survival_time_model(size, train_X, val_X, train_y, val_y):\n",
    "    train_y = preprocess_y_survival(train_y)\n",
    "    val_y = preprocess_y_survival(val_y)\n",
    "    \n",
    "#     svc = SVC(C=5000, kernel='poly', gamma='scale') # sklearn:: Support Vector Classifier\n",
    "#     svc.fit(train_X[:size], train_y[:size])\n",
    "\n",
    "#     score = svc.score(val_X, val_y)\n",
    "#     predict = svc.predict(val_X)\n",
    "    \n",
    "    ### grid search\n",
    "    parameters = {'gamma':('scale', 'auto'), 'kernel':('linear', 'rbf', 'poly'), 'C':[1000, 5000]}\n",
    "    svc = SVC()\n",
    "    clf = GridSearchCV(svc, parameters, n_jobs=-1, cv=5)\n",
    "    clf.fit(train_X[:size], train_y[:size])\n",
    "    print(f'grid search best parameter: {clf.best_params_}')\n",
    "    \n",
    "    score = clf.score(val_X, val_y)\n",
    "    predict = clf.predict(val_X)\n",
    "    \n",
    "    print(f'validation dataset 에 대한 score: {score:.4f}')\n",
    "    print(f'validation dataset 의 분류된 label 수: {len(np.unique(predict))}')\n",
    "    print()\n",
    "    \n",
    "    # 모델 저장 (model/*_model1.pkl)\n",
    "    with open(MODEL1_PATH, 'wb') as fp:\n",
    "        pickle.dump(clf, fp)\n",
    "        \n",
    "def amount_spent_model(size, train_X, val_X, train_y, val_y):\n",
    "    train_y = preprocess_y_spent(train_y)\n",
    "    val_y = preprocess_y_spent(val_y)\n",
    "    \n",
    "#     svc = SVR(C=5000, kernel='linear', gamma='auto') # sklearn:: Support Vector Regressor\n",
    "#     svc.fit(train_X[:size], train_y[:size])\n",
    "\n",
    "#     predict = svc.predict(val_X)\n",
    "#     mse = mean_squared_error(val_y, predict)\n",
    "\n",
    "    ### grid search\n",
    "    parameters = {'gamma':('scale', 'auto'), 'kernel':('linear', 'rbf', 'poly'), 'C':[1000, 5000]}\n",
    "    svr = SVR()\n",
    "    clf = GridSearchCV(svr, parameters, n_jobs=-1, cv=5)\n",
    "    clf.fit(train_X[:size], train_y[:size])\n",
    "    print(f'grid search best parameter: {clf.best_params_}')\n",
    "    \n",
    "    predict = clf.predict(val_X)\n",
    "    mse = mean_squared_error(val_y, predict)\n",
    "    \n",
    "    print(f'validation dataset 에 대한 mse score: {mse:.4f}')\n",
    "    print()\n",
    "    \n",
    "    # 모델 저장 (model/*_model2.pkl)\n",
    "    with open(MODEL2_PATH, 'wb') as fp:\n",
    "        pickle.dump(clf, fp)\n",
    "\n",
    "# 저장된 모델 불러와서 test dataset 에 대해 예측\n",
    "def test_model_svm(train, test1, test2):  \n",
    "    train_y1 = preprocess_y_survival(pd.read_csv(LABEL_PATH))\n",
    "    train_y2 = preprocess_y_spent(pd.read_csv(LABEL_PATH))\n",
    "\n",
    "    with open(MODEL1_PATH, 'rb') as fp:\n",
    "        model = pickle.load(fp)\n",
    "        score = model.score(train, train_y1)\n",
    "        predict1 = model.predict(test1)\n",
    "        predict2 = model.predict(test2)\n",
    "        \n",
    "        print('survival time')\n",
    "        print(f'전체 train dataset 에 대한 score: {score:.4f}')\n",
    "        print(f'분류된 test1 dataset 의 label 수: {len(np.unique(predict1))}')\n",
    "        print(f'분류된 test2 dataset 의 label 수: {len(np.unique(predict2))}')\n",
    "        print()\n",
    "        \n",
    "    with open(MODEL2_PATH, 'rb') as fp:\n",
    "        model = pickle.load(fp)\n",
    "        predict1 = model.predict(test1)\n",
    "        predict2 = model.predict(test2)\n",
    "\n",
    "        print('amount spent')\n",
    "        print(f'predict1: {predict1}')\n",
    "        print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create survival time model\n",
      "grid search best parameter: {'C': 1000, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "validation dataset 에 대한 score: 0.5549\n",
      "validation dataset 의 분류된 label 수: 1\n",
      "\n",
      "create amount spent model\n",
      "grid search best parameter: {'C': 1000, 'gamma': 'auto', 'kernel': 'poly'}\n",
      "validation dataset 에 대한 mse score: 0.4005\n",
      "\n",
      "survival time\n",
      "전체 train dataset 에 대한 score: 0.5499\n",
      "분류된 test1 dataset 의 label 수: 1\n",
      "분류된 test2 dataset 의 label 수: 1\n",
      "\n",
      "amount spent\n",
      "predict1: [0.09987449 0.09989878 0.09992143 ... 0.09907685 0.10061219 0.10035432]\n",
      "\n",
      "Wall time: 1h 58min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ------------------\n",
    "#   main\n",
    "# ------------------\n",
    "\n",
    "train_X = preprocess_X(pd.read_csv(TRAIN_PATH))\n",
    "train_y = pd.read_csv(LABEL_PATH)  # label에 대한 전처리는 각 함수 내에서 작업.\n",
    "test1 = preprocess_X(pd.read_csv(TEST1_PATH))\n",
    "test2 = preprocess_X(pd.read_csv(TEST2_PATH))\n",
    "\n",
    "# scaling\n",
    "# 전체 train 데이터셋에 대해 fit_transform\n",
    "# test 데이터셋에 대해 transform\n",
    "mm = MinMaxScaler()\n",
    "train_X = mm.fit_transform(train_X)\n",
    "test1 = mm.transform(test1)\n",
    "test2 = mm.transform(test2)\n",
    "\n",
    "create_model_svm(train_X, train_y, size=5000) # 실제는 size=40000 으로 train 해야함\n",
    "test_model_svm(train_X, test1, test2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
